This is the GitHub repository for the paper Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages. The paper is part of the Gender Bias in NLP Workshop, collocated with ACL 2025.

The .py script takes two inputs:
1. a csv file generated by the [Python script](https://github.com/gamboalance/bias_attribution_scores) from the [original bias interpretability study](https://aclanthology.org/2024.paclic-1.29/)—e.g., `cp_tl_roberta-tagalog_scores.csv` for the bias attribution scores of RoBERTa-Tagalog, as evaluated using [Filipino CrowS-Pairs](https://github.com/gamboalance/filipino_bias_benchmarks)
2. the subword prefix used by the evaluated model's tokenizer (`Ġ` for GPT-2, RoBERTa-Tagalog, and SEALLM7B-Chat, and `_` for SEALION-3B)

An example of what the output looks like is `sorted_cp_tl_roberta-tagalog.csv`.  
